{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "from sklearn.manifold import trustworthiness\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_iris, load_wine, load_digits, load_breast_cancer, fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, pairwise_distances\n",
    "import scipy.stats as ss\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeCV, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score\n",
    "from typing import Literal, Dict, Tuple, List\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.exceptions import UndefinedMetricWarning  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Gini_MDS import GiniMDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.manifold import trustworthiness\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "def euclidean_distance_matrix(X):\n",
    "    with torch.no_grad():\n",
    "        T = torch.as_tensor(X, dtype=torch.float64)\n",
    "        diff = T[:, None, :] - T[None, :, :]\n",
    "        D = torch.sqrt(torch.clamp((diff ** 2).sum(dim=-1), min=0.0))\n",
    "        return D.cpu().numpy().astype(np.float64)\n",
    "\n",
    "def safe_trustworthiness(X, Z, n_neighbors=5):\n",
    "    try:\n",
    "        return float(trustworthiness(X, Z, n_neighbors=n_neighbors, metric=\"euclidean\"))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def safe_silhouette(Z, y):\n",
    "    try:\n",
    "        y = np.asarray(y)\n",
    "        if len(np.unique(y)) < 2 or len(y) <= len(np.unique(y)):\n",
    "            return np.nan\n",
    "        return float(silhouette_score(Z, y, metric=\"euclidean\"))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def run_gini_mds(X, method, nu, n_components=2, device=DEVICE, dtype=torch.float64,\n",
    "                 gini_mode=\"rank\", max_iter=800, tol=1e-7):\n",
    "    model = GiniMDS(n_components=int(n_components), nu=float(nu),\n",
    "                    gini_mode=gini_mode, mds_method=method,\n",
    "                    dtype=dtype, device=device, max_iter=max_iter, tol=tol, random_state=0)\n",
    "    Z = model.fit_transform(X=X)\n",
    "    D_fit = model.train_D_\n",
    "    s1 = model.stress(D_fit, Z, kind=\"stress1\")\n",
    "    #sS = model.stress(D_fit, Z, kind=\"sammon\")\n",
    "    tw = safe_trustworthiness(X, Z)\n",
    "    return model, Z, s1, tw\n",
    "\n",
    "def run_euclid_mds(X, method, n_components=2, device=DEVICE, dtype=torch.float64,\n",
    "                   iterative_on_cpu=True, max_iter=800, tol=1e-7):\n",
    "    D = euclidean_distance_matrix(X)\n",
    "    dev = device if method == \"cmds\" or not iterative_on_cpu else \"cpu\"\n",
    "    dt  = dtype if method == \"cmds\" else torch.float32\n",
    "    model = GiniMDS(n_components=int(n_components), mds_method=method,\n",
    "                    dtype=dt, device=dev, max_iter=max_iter, tol=tol, random_state=0)\n",
    "    Z = model.fit_transform(D=D)\n",
    "    s1 = model.stress(D, Z, kind=\"stress1\")\n",
    "    #sS = model.stress(D, Z, kind=\"sammon\")\n",
    "    tw = safe_trustworthiness(X, Z)\n",
    "    return model, Z, s1, tw\n",
    "\n",
    "def neighborhood_hit(Z, y, n_neighbors=10):\n",
    "    Z = np.asarray(Z); y = np.ravel(y)\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors+1).fit(Z)\n",
    "    ind = nn.kneighbors(return_distance=False)[:, 1:]  \n",
    "    return float((y[ind] == y[:, None]).mean())\n",
    "\n",
    "def shepard_against(D_ref, Z):\n",
    "    # D_ref: (n,n) distances in original space (fixed for the dataset)\n",
    "    # Z    : (n,m) embedding\n",
    "    DZ = pairwise_distances(Z, metric='euclidean')\n",
    "    iu = np.triu_indices_from(D_ref, k=1)\n",
    "    x = D_ref[iu]; z = DZ[iu]\n",
    "    rho_s, _ = spearmanr(x, z)\n",
    "    rho_p, _ = pearsonr(x, z)\n",
    "    return float(rho_s), float(rho_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST + K FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = (5, 6)\n",
    "DEVICE = \"cuda:1\"\n",
    "DTYPE = torch.float64\n",
    "N_COMPONENTS = 1\n",
    "RANDOM_STATE = 0\n",
    "N_PER_CLASS = 250\n",
    "NOISE_SIGMA = 1\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "def add_noise(X, sigma=0.2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    noisy = X + sigma * rng.standard_normal(X.shape)\n",
    "    return np.clip(noisy, 0.0, 1.0)\n",
    "\n",
    "def extract_by_labels(ds, labels=(1, 7)):\n",
    "    xs, ys = [], []\n",
    "    labset = set(labels)\n",
    "    for x, y in ds:\n",
    "        if y in labset:\n",
    "            xs.append(x.view(-1).numpy())\n",
    "            ys.append(int(y))\n",
    "    X = np.stack(xs, axis=0).astype(np.float32)\n",
    "    y = np.array(ys, dtype=np.int64)\n",
    "    return X, y\n",
    "\n",
    "def stratified_sample(X, y, n_per_class, labels, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Xs, ys = [], []\n",
    "    for lab in labels:\n",
    "        idx = np.where(y == lab)[0]\n",
    "        rng.shuffle(idx)\n",
    "        Xs.append(X[idx[:n_per_class]])\n",
    "        ys.append(np.full(n_per_class, lab, dtype=np.int64))\n",
    "    return np.vstack(Xs), np.hstack(ys)\n",
    "\n",
    "# MNIST\n",
    "print(f\"Loading MNIST (labels {LABELS})...\")\n",
    "to_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=to_tensor)\n",
    "X_full, y_full = extract_by_labels(mnist_train, labels=LABELS)\n",
    "print(f\"Total samples: {len(X_full)} | Per class: {[np.sum(y_full==lab) for lab in LABELS]}\")\n",
    "\n",
    "# K-Folds\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "acc_euc_folds, acc_gini_folds = [], []\n",
    "conf_euc_all, conf_gini_all = [], []\n",
    "y_true_all, y_pred_euc_all, y_pred_gini_all = [], [], []\n",
    "\n",
    "fold_idx = 1\n",
    "for train_idx, test_idx in kf.split(X_full, y_full):\n",
    "    print(f\"\\n=============\")\n",
    "    print(f\" Fold {fold_idx} / 10\")\n",
    "    print(f\"===============\")\n",
    "\n",
    "    X_train_raw, X_test_raw = X_full[train_idx], X_full[test_idx]\n",
    "    y_train_raw, y_test_raw = y_full[train_idx], y_full[test_idx]\n",
    "\n",
    "    # Limit to 250 samples per class for train/test\n",
    "    X_train, y_train = stratified_sample(X_train_raw, y_train_raw, N_PER_CLASS, LABELS, seed=fold_idx)\n",
    "    X_test, y_test = stratified_sample(X_test_raw, y_test_raw, N_PER_CLASS, LABELS, seed=fold_idx + 100)\n",
    "    print(f\"Fold {fold_idx} — X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "    X_train = add_noise(X_train, sigma=NOISE_SIGMA, seed=0)\n",
    "    X_test  = add_noise(X_test,  sigma=NOISE_SIGMA, seed=1)\n",
    "\n",
    "    try:\n",
    "        print(\" Running Euclidean MDS...\")\n",
    "        D_train_euc = pairwise_distances(X_train, metric=\"euclidean\")\n",
    "        euc_cmds = GiniMDS(\n",
    "            n_components=N_COMPONENTS,\n",
    "            mds_method=\"cmds\",\n",
    "            dtype=DTYPE,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        euc_cmds.fit(D=D_train_euc)\n",
    "        Z_train_euc = euc_cmds.embedding_\n",
    "        D_test_train_euc = pairwise_distances(X_test, X_train, metric=\"euclidean\")\n",
    "        Z_test_euc = euc_cmds.fit_inverse_transform(D_test_train_euc)\n",
    "\n",
    "        print(\" Running Gini MDS grid search...\")\n",
    "        nu_grid = np.arange(1.1, 5, 0.1)\n",
    "        best = (None, np.inf, None)\n",
    "        for nu in nu_grid:\n",
    "            model = GiniMDS(\n",
    "                n_components=N_COMPONENTS,\n",
    "                nu=float(nu),\n",
    "                gini_mode=\"rank\",\n",
    "                mds_method=\"cmds\",\n",
    "                dtype=DTYPE,\n",
    "                device=DEVICE,  # ok for RTX 8000 with 500 samples\n",
    "                random_state=RANDOM_STATE,\n",
    "            )\n",
    "            model.fit(X_train)\n",
    "            stress = model.stress(model.train_D_, model.embedding_)\n",
    "            if stress < best[1]:\n",
    "                best = (nu, stress, model)\n",
    "\n",
    "        nu_star, _, model_gini = best\n",
    "        print(f\"Best nu for fold {fold_idx}: {nu_star:.2f}\")\n",
    "        Z_train_gini = model_gini.embedding_\n",
    "\n",
    "        # Nyström for test set (only 1000 samples total)\n",
    "        X_all = np.vstack([X_train, X_test])\n",
    "        with torch.no_grad():\n",
    "            D_all = model_gini._gini_distance_matrix(\n",
    "                torch.as_tensor(X_all, device=model_gini.device, dtype=model_gini.dtype)\n",
    "            ).cpu().numpy()\n",
    "        D_test_train_gini = D_all[len(X_train):, :len(X_train)]\n",
    "        Z_test_gini = model_gini.fit_inverse_transform(D_test_train_gini)\n",
    "\n",
    "        # Voting\n",
    "        def make_voter():\n",
    "            clf_mlp = MLPClassifier(hidden_layer_sizes=(64,),max_iter=500,random_state=RANDOM_STATE)\n",
    "            clf_log = LogisticRegression(max_iter=1000,random_state=RANDOM_STATE)\n",
    "            clf_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "            clf_lda = LinearDiscriminantAnalysis()\n",
    "            clf_qda = QuadraticDiscriminantAnalysis()\n",
    "            clf_svm = LinearSVC(random_state=RANDOM_STATE)\n",
    "            clf_tree = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "            clf_nb = GaussianNB()\n",
    "            clf_ridge = RidgeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "            return VotingClassifier(\n",
    "                estimators=[\n",
    "                    (\"mlp\", clf_mlp),\n",
    "                    (\"logreg\", clf_log),\n",
    "                    (\"knn\", clf_knn),\n",
    "                    (\"lda\", clf_lda),\n",
    "                    (\"qda\", clf_qda),\n",
    "                    (\"svm\", clf_svm),\n",
    "                    (\"tree\", clf_tree),\n",
    "                    (\"ridge\", clf_ridge),\n",
    "                    (\"nb\", clf_nb),],\n",
    "                voting=\"hard\")\n",
    "\n",
    "        voting_euc = make_voter()\n",
    "        voting_euc.fit(Z_train_euc, y_train)\n",
    "        y_pred_euc = voting_euc.predict(Z_test_euc)\n",
    "\n",
    "        voting_gini = make_voter()\n",
    "        voting_gini.fit(Z_train_gini, y_train)\n",
    "        y_pred_gini = voting_gini.predict(Z_test_gini)\n",
    "\n",
    "        # METRICS\n",
    "        acc_euc = accuracy_score(y_test, y_pred_euc)\n",
    "        acc_gini = accuracy_score(y_test, y_pred_gini)\n",
    "        conf_euc = confusion_matrix(y_test, y_pred_euc, labels=LABELS)\n",
    "        conf_gini = confusion_matrix(y_test, y_pred_gini, labels=LABELS)\n",
    "        print(f\"Fold {fold_idx} — Euclidean acc: {acc_euc:.4f}, Gini acc: {acc_gini:.4f}\")\n",
    "\n",
    "        acc_euc_folds.append(acc_euc)\n",
    "        acc_gini_folds.append(acc_gini)\n",
    "        conf_euc_all.append(conf_euc)\n",
    "        conf_gini_all.append(conf_gini)\n",
    "        y_true_all.extend(y_test)\n",
    "        y_pred_euc_all.extend(y_pred_euc)\n",
    "        y_pred_gini_all.extend(y_pred_gini)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on fold {fold_idx}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # GPU CLEANUP\n",
    "        try:\n",
    "            for obj_name in [\"euc_cmds\", \"model_gini\"]:\n",
    "                if obj_name in locals():\n",
    "                    obj = locals()[obj_name]\n",
    "                    if hasattr(obj, \"embedding_\") and torch.is_tensor(obj.embedding_):\n",
    "                        obj.embedding_ = obj.embedding_.cpu()\n",
    "                    if hasattr(obj, \"train_D_\") and torch.is_tensor(obj.train_D_):\n",
    "                        obj.train_D_ = obj.train_D_.cpu()\n",
    "                    del obj\n",
    "\n",
    "            for var in [\n",
    "                \"D_train_euc\",\n",
    "                \"D_all\",\n",
    "                \"Z_train_euc\",\n",
    "                \"Z_test_euc\",\n",
    "                \"Z_train_gini\",\n",
    "                \"Z_test_gini\",\n",
    "                \"X_train\",\n",
    "                \"X_test\",\n",
    "                \"y_train\",\n",
    "                \"y_test\",\n",
    "            ]:\n",
    "                if var in locals():\n",
    "                    del locals()[var]\n",
    "\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.ipc_collect()\n",
    "\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"GPU cleanup issue (ignored safely): {cleanup_error}\")\n",
    "\n",
    "        print(f\"GPU memory cleared after fold {fold_idx}\")\n",
    "        fold_idx += 1\n",
    "\n",
    "# Results\n",
    "print(\"\\n==========================\")\n",
    "print(\" FINAL RESULTS (10 FOLDS)\")\n",
    "print(\"============================\")\n",
    "\n",
    "if len(acc_euc_folds) > 0 and len(acc_gini_folds) > 0:\n",
    "    print(f\"Mean Euclidean acc: {np.mean(acc_euc_folds):.4f}\")\n",
    "    print(f\"Mean Gini acc:      {np.mean(acc_gini_folds):.4f}\")\n",
    "\n",
    "if len(y_true_all) > 0:\n",
    "    final_conf_euc = np.sum(conf_euc_all, axis=0)\n",
    "    final_conf_gini = np.sum(conf_gini_all, axis=0)\n",
    "\n",
    "    print(\"\\n Final Euclidean Confusion Matrix:\")\n",
    "    print(final_conf_euc)\n",
    "    print(\"\\n Final Gini Confusion Matrix:\")\n",
    "    print(final_conf_gini)\n",
    "\n",
    "    print(\"\\n Euclidean Classification Report (Aggregated):\")\n",
    "    print(classification_report(y_true_all, y_pred_euc_all, labels=LABELS, digits=4, zero_division=0))\n",
    "\n",
    "    print(\"\\n Gini Classification Report (Aggregated):\")\n",
    "    print(classification_report(y_true_all, y_pred_gini_all, labels=LABELS, digits=4, zero_division=0))\n",
    "else:\n",
    "    print(\"⚠️ No folds completed successfully — no results to report.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINIST without 10-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST MDS pipeline (parametric labels) with noise injection, GPU device cuda:1\n",
    "# - Uses your GiniMDS class (already defined in your environment)\n",
    "# - Handles arbitrary label pairs (e.g., (3,5), (4,6), (1,7))\n",
    "# - Adds Gaussian noise to MNIST samples\n",
    "# - Avoids duplicate MDS/Nyström helpers by using GiniMDS(cmds) API\n",
    "# - Includes safety checks for empty/insufficient samples\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import pairwise_distances, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ----------------------\n",
    "# Config \n",
    "# ----------------------\n",
    "LABELS = (5, 6)           # <-- change this to any pair, e.g., (4, 6) or (1, 7)\n",
    "# 4-6 no 1-7 no 1-4 no 3-5 no\n",
    "# 5-6 ok 6-8 ok\n",
    "\n",
    "N_PER_CLASS_TRAIN = 250\n",
    "N_PER_CLASS_TEST  = 250\n",
    "DEVICE = \"cuda:0\"\n",
    "DTYPE  = torch.float64\n",
    "N_COMPONENTS = 2\n",
    "RANDOM_STATE = 0\n",
    "NOISE_SIGMA = 0\n",
    "\n",
    "# ----------------------\n",
    "# Data helpers\n",
    "# ----------------------\n",
    "def extract_by_labels(ds, labels=(1, 7)):\n",
    "    xs, ys = [], []\n",
    "    labset = set(labels)\n",
    "    for x, y in ds:\n",
    "        if y in labset:\n",
    "            xs.append(x.view(-1).numpy())  # 28x28 -> 784\n",
    "            ys.append(int(y))\n",
    "    if not xs:\n",
    "        raise ValueError(f\"No samples found for labels {labels}. \"\n",
    "                         f\"Make sure the dataset split actually contains them.\")\n",
    "    X = np.stack(xs, axis=0).astype(np.float32)  # values in [0,1]\n",
    "    y = np.array(ys, dtype=np.int64)\n",
    "    return X, y\n",
    "\n",
    "def stratified_sample(X, y, n_per_class, labels=(1, 7), seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Xs, ys = [], []\n",
    "    for lab in labels:\n",
    "        idx = np.where(y == lab)[0]\n",
    "        if len(idx) < n_per_class:\n",
    "            raise ValueError(\n",
    "                f\"Not enough samples for label {lab}: requested {n_per_class}, \"\n",
    "                f\"but only {len(idx)} available.\"\n",
    "            )\n",
    "        rng.shuffle(idx)\n",
    "        Xs.append(X[idx[:n_per_class]])\n",
    "        ys.append(np.full(n_per_class, lab, dtype=np.int64))\n",
    "    return np.vstack(Xs), np.hstack(ys)\n",
    "\n",
    "def add_noise(X, sigma=0.2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    noisy = X + sigma * rng.standard_normal(X.shape)\n",
    "    return np.clip(noisy, 0.0, 1.0)\n",
    "\n",
    "# ----------------------\n",
    "# Load MNIST and extract labels\n",
    "# ----------------------\n",
    "print(f\"Loading MNIST and keeping labels {LABELS}...\")\n",
    "to_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True,  download=True, transform=to_tensor)\n",
    "mnist_test  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=to_tensor)\n",
    "\n",
    "X_train_full, y_train_full = extract_by_labels(mnist_train, labels=LABELS)\n",
    "X_test_full,  y_test_full  = extract_by_labels(mnist_test,  labels=LABELS)\n",
    "\n",
    "X_train, y_train = stratified_sample(X_train_full, y_train_full, N_PER_CLASS_TRAIN, labels=LABELS, seed=0)\n",
    "X_test,  y_test  = stratified_sample(X_test_full,  y_test_full,  N_PER_CLASS_TEST,  labels=LABELS, seed=1)\n",
    "\n",
    "# Add Gaussian noise\n",
    "X_train = add_noise(X_train, sigma=NOISE_SIGMA, seed=0)\n",
    "X_test  = add_noise(X_test,  sigma=NOISE_SIGMA, seed=1)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "print(f\"Train: {n_train} samples | Test: {X_test.shape[0]} samples (with noise)\")\n",
    "\n",
    "# ----------------------\n",
    "# Euclidean MDS via GiniMDS('cmds') fit on Euclidean D\n",
    "# ----------------------\n",
    "print(\"Running Euclidean MDS (classical MDS on Euclidean distances) ...\")\n",
    "D_train_euc = pairwise_distances(X_train, metric=\"euclidean\")\n",
    "\n",
    "euc_cmds = GiniMDS(\n",
    "    n_components=N_COMPONENTS,\n",
    "    mds_method='cmds',\n",
    "    dtype=DTYPE,\n",
    "    device=DEVICE\n",
    ")\n",
    "euc_cmds.fit(D=D_train_euc)\n",
    "Z_train_euc = euc_cmds.embedding_\n",
    "\n",
    "D_test_train_euc = pairwise_distances(X_test, X_train, metric=\"euclidean\")\n",
    "Z_test_euc = euc_cmds.fit_inverse_transform(D_test_train_euc)\n",
    "\n",
    "# ----------------------\n",
    "# Gini MDS grid search (fit on X; model computes Gini distances internally)\n",
    "# ----------------------\n",
    "print(\"Running Gini MDS grid search...\")\n",
    "nu_grid = np.arange(1.1, 4.1, 0.1)\n",
    "best = (None, np.inf, None)  # (nu, stress, model)\n",
    "\n",
    "for nu in nu_grid:\n",
    "    model = GiniMDS(\n",
    "        n_components=N_COMPONENTS,\n",
    "        nu=float(nu),\n",
    "        gini_mode='rank',\n",
    "        mds_method='cmds',\n",
    "        dtype=DTYPE,\n",
    "        device=DEVICE,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    model.fit(X_train)\n",
    "    stress = model.stress(model.train_D_, model.embedding_)\n",
    "    print(f\"nu={nu:.2f} -> stress1={stress:.6f}\")\n",
    "    if stress < best[1]:\n",
    "        best = (nu, stress, model)\n",
    "\n",
    "nu_star, _, model_gini = best\n",
    "Z_train_gini = model_gini.embedding_\n",
    "print(f\"Best nu for Gini MDS: {nu_star:.2f}\")\n",
    "\n",
    "# ----------------------\n",
    "# Gini Nyström: use Gini distances (same metric as training)\n",
    "# ----------------------\n",
    "print(\"Computing Gini test–train distances for Nyström...\")\n",
    "X_all = np.vstack([X_train, X_test])\n",
    "with torch.no_grad():\n",
    "    D_all = model_gini._gini_distance_matrix(\n",
    "        torch.as_tensor(X_all, device=model_gini.device, dtype=model_gini.dtype)\n",
    "    ).cpu().numpy()\n",
    "D_test_train_gini = D_all[n_train:, :n_train]\n",
    "Z_test_gini = model_gini.fit_inverse_transform(D_test_train_gini)\n",
    "\n",
    "# ----------------------\n",
    "# Classify \n",
    "# ----------------------\n",
    "clf_euc = MLPClassifier(hidden_layer_sizes=(64,), max_iter=500, random_state=RANDOM_STATE)\n",
    "clf_euc.fit(Z_train_euc, y_train)\n",
    "y_pred_euc = clf_euc.predict(Z_test_euc)\n",
    "\n",
    "clf_gini = MLPClassifier(hidden_layer_sizes=(64,), max_iter=500, random_state=RANDOM_STATE)\n",
    "clf_gini.fit(Z_train_gini, y_train)\n",
    "y_pred_gini = clf_gini.predict(Z_test_gini)\n",
    "\n",
    "#print(\"\\n Euclidean MDS — Accuracy:\", accuracy_score(y_test, y_pred_euc))\n",
    "#print(classification_report(y_test, y_pred_euc, digits=4))\n",
    "#print(\"Confusion matrix (Euclidean):\")\n",
    "#print(confusion_matrix(y_test, y_pred_euc))\n",
    "\n",
    "#print(\"\\n Gini MDS (nu={:.2f}) — Accuracy:\".format(nu_star), accuracy_score(y_test, y_pred_gini))\n",
    "#print(classification_report(y_test, y_pred_gini, digits=4))\n",
    "#print(\"Confusion matrix (Gini):\")\n",
    "#print(confusion_matrix(y_test, y_pred_gini))\n",
    "\n",
    "# Define individual classifiers\n",
    "logreg_euc = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "knn_euc = KNeighborsClassifier(n_neighbors=5)\n",
    "logreg_gini = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "knn_gini = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train each base classifier\n",
    "logreg_euc.fit(Z_train_euc, y_train)\n",
    "knn_euc.fit(Z_train_euc, y_train)\n",
    "logreg_gini.fit(Z_train_gini, y_train)\n",
    "knn_gini.fit(Z_train_gini, y_train)\n",
    "\n",
    "# Vote Euclidean\n",
    "voting_euc = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('mlp', clf_euc),\n",
    "        ('logreg', logreg_euc),\n",
    "        ('knn', knn_euc)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_euc.fit(Z_train_euc, y_train)\n",
    "y_pred_vote_euc = voting_euc.predict(Z_test_euc)\n",
    "\n",
    "print(\"\\n Euclidean VotingClassifier — Accuracy:\", accuracy_score(y_test, y_pred_vote_euc))\n",
    "print(classification_report(y_test, y_pred_vote_euc, digits=4))\n",
    "print(\"Confusion matrix (Euclidean Voting):\")\n",
    "print(confusion_matrix(y_test, y_pred_vote_euc))\n",
    "\n",
    "# Vote Gini\n",
    "voting_gini = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('mlp', clf_gini),\n",
    "        ('logreg', logreg_gini),\n",
    "        ('knn', knn_gini)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_gini.fit(Z_train_gini, y_train)\n",
    "y_pred_vote_gini = voting_gini.predict(Z_test_gini)\n",
    "\n",
    "print(\"\\n Gini VotingClassifier — Accuracy:\", accuracy_score(y_test, y_pred_vote_gini))\n",
    "print(classification_report(y_test, y_pred_vote_gini, digits=4))\n",
    "print(\"Confusion matrix (Gini Voting):\")\n",
    "print(confusion_matrix(y_test, y_pred_vote_gini))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_samples_6_and_8(X, y, labels=(6, 8), n_per_class=3):\n",
    "    \"\"\"Display n samples of digits 6 and 8: top row = first digit, bottom = second.\"\"\"\n",
    "    fig, axes = plt.subplots(2, n_per_class, figsize=(n_per_class * 2, 4))\n",
    "\n",
    "    for row, label in enumerate(labels):\n",
    "        idx = np.where(y == label)[0][:n_per_class]\n",
    "        for col, i in enumerate(idx):\n",
    "            ax = axes[row, col]\n",
    "            ax.imshow(X[i].reshape(28, 28), cmap='viridis')\n",
    "            ax.set_title(f\"{label}\", fontsize=10)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"MNIST samples for digits {labels[0]} and {labels[1]}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "LABELS = (5, 6)  \n",
    "X_train_full, y_train_full = extract_by_labels(mnist_train, labels=LABELS)\n",
    "show_samples_6_and_8(X_train_full, y_train_full, labels=LABELS, n_per_class=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_noise(X, sigma=0.2, seed=0):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to flattened MNIST images.\n",
    "    X: (n_samples, 784)\n",
    "    sigma: noise standard deviation\n",
    "    seed: random seed\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    noisy = X + sigma * rng.standard_normal(X.shape)\n",
    "    return np.clip(noisy, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def show_samples_with_noise(X, y, labels=(6, 8), n_per_class=3, sigma=0.2, seed=0):\n",
    "    \"\"\"\n",
    "    Display clean samples (top) and noisy samples (bottom) for the given labels.\n",
    "    Top 2 rows = clean images (for each label)\n",
    "    Bottom 2 rows = noisy images (for each label)\n",
    "    \"\"\"\n",
    "    # Add noise\n",
    "    X_noisy = add_noise(X, sigma=sigma, seed=seed)\n",
    "\n",
    "    # Select indices for each label\n",
    "    idx_clean = []\n",
    "    for lab in labels:\n",
    "        idx_lab = np.where(y == lab)[0][:n_per_class]\n",
    "        idx_clean.extend(idx_lab)\n",
    "\n",
    "    fig, axes = plt.subplots(2, len(idx_clean), figsize=(len(idx_clean) * 1.8, 4))\n",
    "    plt.suptitle(\n",
    "        f\"MNIST digits {labels[0]} and {labels[1]}\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    for col, i in enumerate(idx_clean):\n",
    "        # Top row: clean images\n",
    "        axes[0, col].imshow(X[i].reshape(28, 28), cmap=\"viridis\")\n",
    "        axes[0, col].set_title(f\"Clean {y[i]}\", fontsize=9)\n",
    "        axes[0, col].axis(\"off\")\n",
    "\n",
    "        # Bottom row: noisy images\n",
    "        axes[1, col].imshow(X_noisy[i].reshape(28, 28), cmap=\"viridis\")\n",
    "        axes[1, col].set_title(f\"Noisy {y[i]}\", fontsize=9)\n",
    "        axes[1, col].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.82)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "LABELS = (5, 6)\n",
    "X_train_full, y_train_full = extract_by_labels(mnist_train, labels=LABELS)\n",
    "show_samples_with_noise(X_train_full, y_train_full, labels=LABELS, n_per_class=2, sigma=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import pairwise_distances, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "\n",
    "# ----------------------\n",
    "# Step 1: Load Digits (Digits 1 & 7)\n",
    "# ----------------------\n",
    "print(\"Loading Digits dataset (digits 1 and 7)...\")\n",
    "digits = load_digits()\n",
    "X_raw = digits.data / 16.0  # Normalize (digits pixel values are 0–16)\n",
    "y_raw = digits.target\n",
    "\n",
    "# Keep only digits 1 and 7\n",
    "mask = (y_raw == 6) | (y_raw == 8)\n",
    "X = X_raw[mask]\n",
    "y = y_raw[mask]\n",
    "\n",
    "# ----------------------\n",
    "# Step 2: Stratified Sampling\n",
    "# ----------------------\n",
    "def stratified_sample(X, y, n_per_class, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X_out, y_out = [], []\n",
    "    for label in [6, 8]:\n",
    "        idx = np.where(y == label)[0]\n",
    "        rng.shuffle(idx)\n",
    "        X_out.append(X[idx[:n_per_class]])\n",
    "        y_out.append(np.full(n_per_class, label))\n",
    "    return np.vstack(X_out), np.hstack(y_out)\n",
    "\n",
    "X_train, y_train = stratified_sample(X, y, 50, seed=0)\n",
    "X_test, y_test = stratified_sample(X[100:], y[100:], 50, seed=1)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# ----------------------\n",
    "# Step 3: Euclidean MDS (classical) using GiniMDS('cmds') on Euclidean D\n",
    "# ----------------------\n",
    "print(\"Running Euclidean MDS with GiniMDS(cmds)...\")\n",
    "D_train_euc = pairwise_distances(X_train, metric=\"euclidean\")\n",
    "\n",
    "euc_cmds = GiniMDS(\n",
    "    n_components=3,\n",
    "    mds_method='cmds',\n",
    "    dtype=torch.float64,\n",
    "    device=\"cuda:1\"   # Force GPU usage\n",
    ")\n",
    "euc_cmds.fit(D=D_train_euc)\n",
    "Z_train_euc = euc_cmds.embedding_\n",
    "\n",
    "D_test_train_euc = pairwise_distances(X_test, X_train, metric=\"euclidean\")\n",
    "Z_test_euc = euc_cmds.fit_inverse_transform(D_test_train_euc)\n",
    "\n",
    "# ----------------------\n",
    "# Step 4: Gini MDS with ν Grid Search\n",
    "# ----------------------\n",
    "print(\"Running Gini MDS grid search...\")\n",
    "nu_grid = np.arange(1.1, 3.1, 0.1)\n",
    "best = (None, np.inf, None)  # (nu, stress, model)\n",
    "\n",
    "for nu in nu_grid:\n",
    "    model = GiniMDS(\n",
    "        n_components=3,\n",
    "        nu=float(nu),\n",
    "        gini_mode='rank',\n",
    "        mds_method='cmds',\n",
    "        dtype=torch.float64,\n",
    "        device=\"cuda:1\"   # Force GPU usage\n",
    "    )\n",
    "    model.fit(X_train)\n",
    "    stress = model.stress(model.train_D_, model.embedding_)\n",
    "    print(f\"nu={nu:.2f},  stress1={stress:.6f}\")\n",
    "    if stress < best[1]:\n",
    "        best = (nu, stress, model)\n",
    "\n",
    "nu_star, _, model_gini = best\n",
    "Z_train_gini = model_gini.embedding_\n",
    "print(f\" Best nu for Gini MDS: {nu_star:.2f}\")\n",
    "\n",
    "# ----------------------\n",
    "# Step 5: Project test points using Gini's fit_inverse_transform\n",
    "# ----------------------\n",
    "print(\"Computing Gini test–train distances for Nyström...\")\n",
    "X_all = np.vstack([X_train, X_test])\n",
    "with torch.no_grad():\n",
    "    D_all = model_gini._gini_distance_matrix(\n",
    "        torch.as_tensor(X_all, device=model_gini.device, dtype=model_gini.dtype)\n",
    "    ).cpu().numpy()\n",
    "D_test_train_gini = D_all[n_train:, :n_train]\n",
    "Z_test_gini = model_gini.fit_inverse_transform(D_test_train_gini)\n",
    "\n",
    "# ----------------------\n",
    "# Step 6: Train and Evaluate MLP Classifier\n",
    "# ----------------------\n",
    "clf_euc = MLPClassifier(hidden_layer_sizes=(32,), max_iter=500, random_state=0)\n",
    "clf_euc.fit(Z_train_euc, y_train)\n",
    "y_pred_euc = clf_euc.predict(Z_test_euc)\n",
    "\n",
    "clf_gini = MLPClassifier(hidden_layer_sizes=(32,), max_iter=500, random_state=0)\n",
    "clf_gini.fit(Z_train_gini, y_train)\n",
    "y_pred_gini = clf_gini.predict(Z_test_gini)\n",
    "\n",
    "print(\"\\n Euclidean MDS — Accuracy:\", accuracy_score(y_test, y_pred_euc))\n",
    "print(classification_report(y_test, y_pred_euc, digits=4))\n",
    "print(\"Confusion matrix (Euclidean):\")\n",
    "print(confusion_matrix(y_test, y_pred_euc))\n",
    "\n",
    "print(\"\\n Gini MDS (nu={:.2f}) — Accuracy:\".format(nu_star), accuracy_score(y_test, y_pred_gini))\n",
    "print(classification_report(y_test, y_pred_gini, digits=4))\n",
    "print(\"Confusion matrix (Gini):\")\n",
    "print(confusion_matrix(y_test, y_pred_gini))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_digits_6_and_8(X, y, n_each=3):\n",
    "    \"\"\"Show n samples of digits 6 and 8 from the dataset.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2 * n_each, figsize=(12, 2))\n",
    "\n",
    "    idx_6 = np.where(y == 6)[0][:n_each]\n",
    "    idx_8 = np.where(y == 8)[0][:n_each]\n",
    "\n",
    "    selected = np.concatenate([idx_6, idx_8])\n",
    "    for i, idx in enumerate(selected):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(X[idx].reshape(8, 8), cmap='gray')  # 8x8 for sklearn digits\n",
    "        ax.set_title(f\"Label: {y[idx]}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Digits 6 and 8\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "show_digits_6_and_8(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest         # robust covariance (Mahalanobis)\n",
    "from outliers import smirnov_grubbs as grubbs\n",
    "\n",
    "for ds_name, loader in DATASETS.items():\n",
    "    X, y = loader()\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    clf = IForest(contamination=0.01, random_state=0)          # or IForest(), LOF(), etc.\n",
    "    clf.fit(X)\n",
    "    labels = clf.labels_            # 1 = outlier, 0 = inlier\n",
    "    scores = clf.decision_scores_\n",
    "    n_outliers = labels.sum()\n",
    "    #outliers = grubbs.test(X[:, 0], alpha=0.05)\n",
    "    print(f\"{ds_name}\\n{n_outliers}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# from datasets import load_iono\n",
    "# from gini_mds import run_gini_mds, run_euclid_mds\n",
    "\n",
    "# ---- Load and prepare data\n",
    "X, y = load_bank()\n",
    "X = np.asarray(X, dtype=np.float64)\n",
    "\n",
    "# --- Add noise to 5% of X\n",
    "num_samples = X.shape[0]\n",
    "num_noisy = int(0.05 * num_samples)\n",
    "noisy_indices = np.random.choice(num_samples, size=num_noisy, replace=False)\n",
    "noise_std = 10 * np.std(X)\n",
    "noise = np.random.normal(loc=0.0, scale=noise_std, size=(num_noisy, X.shape[1]))\n",
    "X_noisy = X.copy()\n",
    "X_noisy[noisy_indices] += noise\n",
    "\n",
    "# --- Reference distances\n",
    "D_ref = pairwise_distances(X_noisy, metric='euclidean')\n",
    "\n",
    "# ---- Parameters\n",
    "nu_grid = np.arange(1.1, 4.0 + 1e-9, 0.1)\n",
    "gini_method = \"cmds\"\n",
    "m = 3  # components\n",
    "\n",
    "# ---- 1️ Gini MDS grid search for best ν\n",
    "t0 = time.perf_counter()\n",
    "best = (None, np.inf)\n",
    "for nu in nu_grid:\n",
    "    _, Z_g, s1_g, _ = run_gini_mds(X_noisy, gini_method, nu, n_components=m)\n",
    "    if s1_g < best[1]:\n",
    "        best = (nu, s1_g)\n",
    "t_gini = time.perf_counter() - t0\n",
    "best_nu = best[0]\n",
    "\n",
    "# ---- 2️ Euclidean MDS (torch-based, via Gini module)\n",
    "t1 = time.perf_counter()\n",
    "_, Z_e_torch, s1_e_torch, _ = run_euclid_mds(X_noisy, \"cmds\", n_components=m)\n",
    "t_eucl_torch = time.perf_counter() - t1\n",
    "\n",
    "# ---- 3️ Euclidean MDS (sklearn)\n",
    "t2 = time.perf_counter()\n",
    "std_mds = MDS(n_components=m, dissimilarity='euclidean', random_state=42)\n",
    "_ = std_mds.fit_transform(X_noisy)\n",
    "t_sklearn = time.perf_counter() - t2\n",
    "\n",
    "# ---- Print results\n",
    "print(\"\\n=== Execution Times (Ionosphere, 3 components) ===\")\n",
    "print(f\"Gini MDS (best nu={best_nu:.2f}): {t_gini:.3f} s\")\n",
    "print(f\"Euclidean MDS (torch):           {t_eucl_torch:.3f} s\")\n",
    "print(f\"Euclidean MDS (sklearn):         {t_sklearn:.3f} s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
